# **Evaluating Pre-Trained Models for Text Conversations Using TOPSIS**

## 📌 **Objective**
This project applies the **TOPSIS** (Technique for Order Preference by Similarity to Ideal Solution) method to evaluate and rank different **pre-trained conversational AI models**. The evaluation is based on various **performance metrics**.

## 🛠 **Models Evaluated**
- **microsoft/DialoGPT-medium**
- **facebook/blenderbot-400M-distill**
- **google/flan-t5-base**
- **t5-small**

## 📊 **Evaluation Metrics**
The models are compared using the following metrics:
1. **Perplexity** (Lower is better)
2. **BLEU Score** (Higher is better)
3. **ROUGE Score** (Higher is better)
4. **TOPSIS Score** (Final ranking metric)

## 🔢 **Results Table**
<img width="462" alt="Screenshot 2025-02-01 at 4 41 22 PM" src="https://github.com/user-attachments/assets/c95a4634-6ef5-472b-ba32-dad30ea7b515" />

## 📈 **Graphs & Visualizations**
### **1️⃣ Performance Comparison**
- 📊 **Bar charts** for comparing individual scores
  <img width="987" alt="Screenshot 2025-02-01 at 4 39 52 PM" src="https://github.com/user-attachments/assets/0b7c4663-6edd-424d-bc79-b49f239ca35f" />
  
- 📉 **TOPSIS Score line graph** to visualize ranking trends
  
  <img width="479" alt="Screenshot 2025-02-01 at 4 40 17 PM" src="https://github.com/user-attachments/assets/e950654a-e0d9-420a-b935-160573dd66bc" />


