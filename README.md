# **Evaluating Pre-Trained Models for Text Conversations Using TOPSIS**

## ğŸ“Œ **Objective**
This project applies the **TOPSIS** (Technique for Order Preference by Similarity to Ideal Solution) method to evaluate and rank different **pre-trained conversational AI models**. The evaluation is based on various **performance metrics**.

## ğŸ›  **Models Evaluated**
- **microsoft/DialoGPT-medium**
- **facebook/blenderbot-400M-distill**
- **google/flan-t5-base**
- **t5-small**

## ğŸ“Š **Evaluation Metrics**
The models are compared using the following metrics:
1. **Perplexity** (Lower is better)
2. **BLEU Score** (Higher is better)
3. **ROUGE Score** (Higher is better)
4. **TOPSIS Score** (Final ranking metric)

## ğŸ”¢ **Results Table**
<img width="462" alt="Screenshot 2025-02-01 at 4 41 22â€¯PM" src="https://github.com/user-attachments/assets/c95a4634-6ef5-472b-ba32-dad30ea7b515" />

## ğŸ“ˆ **Graphs & Visualizations**
### **1ï¸âƒ£ Performance Comparison**
- ğŸ“Š **Bar charts** for comparing individual scores
  <img width="987" alt="Screenshot 2025-02-01 at 4 39 52â€¯PM" src="https://github.com/user-attachments/assets/0b7c4663-6edd-424d-bc79-b49f239ca35f" />
  
- ğŸ“‰ **TOPSIS Score line graph** to visualize ranking trends
  
  <img width="479" alt="Screenshot 2025-02-01 at 4 40 17â€¯PM" src="https://github.com/user-attachments/assets/e950654a-e0d9-420a-b935-160573dd66bc" />


